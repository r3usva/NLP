{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "600c6a8d-cd4a-4208-96af-038a5625d315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\sampa\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (2.3.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sampa\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.5.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.1-cp311-cp311-win_amd64.whl (8.9 MB)\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.6/8.9 MB 9.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.4/8.9 MB 8.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.5/8.9 MB 9.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.6/8.9 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.1/8.9 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.9/8.9 MB 7.7 MB/s  0:00:01\n",
      "Downloading scipy-1.16.1-cp311-cp311-win_amd64.whl (38.6 MB)\n",
      "   ---------------------------------------- 0.0/38.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.6/38.6 MB 8.4 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 3.7/38.6 MB 8.7 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 5.2/38.6 MB 8.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 7.1/38.6 MB 8.9 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 9.2/38.6 MB 9.1 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 11.0/38.6 MB 9.1 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 13.1/38.6 MB 9.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 14.9/38.6 MB 9.2 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 16.8/38.6 MB 9.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 18.9/38.6 MB 9.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 20.7/38.6 MB 9.2 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 22.8/38.6 MB 9.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 24.9/38.6 MB 9.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 26.7/38.6 MB 9.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 28.8/38.6 MB 9.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 31.2/38.6 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.6/38.6 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.7/38.6 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.5/38.6 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.6 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.6/38.6 MB 9.1 MB/s  0:00:04\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   ---------------------------------------- 3/3 [scikit-learn]\n",
      "\n",
      "Successfully installed scikit-learn-1.7.1 scipy-1.16.1 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb9721e0-e55f-4253-a8e5-8f171ae5c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f74faa7-a55b-4184-b865-ad45bffd15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "posts = fetch_20newsgroups(subset = 'all', categories =['sci.electronics', 'sci.space'], remove = ('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1b14048-f943-4f28-b40b-4b2ffba08445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "#See the available keys in dataset\n",
    "print(posts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d96785d-b64d-4c10-b1a9-85bfecd30929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL>>        Question:   Is there a certain device out there that I can\n",
      "AL>>                    use to find out the number to the line?\n",
      "AL>>        Thanks for any response.\n",
      "AL>>                                                    Al\n",
      "\n",
      "AL>There is a number you can call which will return a synthesized\n",
      "AL>voice telling you the number of the line.  Unfortunately, for the\n",
      "AL>life of me I can't remember what it is. The telephone technicians\n",
      "AL>use it all the time.  We used to play around with this in our\n",
      "AL>dorm rooms since there were multiple phone lines running between\n",
      "AL>rooms.\n",
      "\n",
      "It probably wouldn't help for you to post the number, since it appears\n",
      "to be different in each area.  For what it's worth, in the New Orleans\n",
      "area the number is 998-877-6655 (easy to remember, what?)\n",
      "\n",
      "\n",
      " * SLMR 2.1 * Ask me anything: if I don't know, I'll make up something.\n",
      "                                          \n"
     ]
    }
   ],
   "source": [
    "#Display One data point\n",
    "print(posts.data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bd25852-5d43-4551-a3c9-3ff7f3df5a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a data Frame\n",
    "df = pd.DataFrame({\n",
    "    'text': posts.data,\n",
    "    'label': [posts.target_names[target] for target in posts.target]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b582a45-2143-4381-850f-857f8bc21e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text            label\n",
      "0  \\n   >\\tIf the  new  Kuiper belt object *is*  ...        sci.space\n",
      "1  AL>>        Question:   Is there a certain dev...  sci.electronics\n",
      "2  \\nIt's not quite what you were asking, but a f...        sci.space\n",
      "3  \\n\\n\\nNo, the sky does not, at this time, belo...        sci.space\n",
      "4   \\nDigi-Key also sells Quad Line Receivers, pa...  sci.electronics\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "441d7c4a-c192-4acc-b105-a480d3b67005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1971, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07121b0d-6933-4d51-bffb-3bf7de1dc5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sampa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sampa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sampa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Cleaning and Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b201747-2061-4784-b47c-f36e182224a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    #Remove tokens that are not purely letters\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "    #Lowercase the text\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    #Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    #Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    #Join tokens back into a single string\n",
    "    clean_text = ' '.join(tokens)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7795217-fbd3-4458-b9b9-99a323d2d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply this above clean_text function to the dataset\n",
    "df['clean_text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad2d4e6b-d254-48ca-915d-7efaf9df0a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text            label  \\\n",
      "0  \\n   >\\tIf the  new  Kuiper belt object *is*  ...        sci.space   \n",
      "1  AL>>        Question:   Is there a certain dev...  sci.electronics   \n",
      "2  \\nIt's not quite what you were asking, but a f...        sci.space   \n",
      "3  \\n\\n\\nNo, the sky does not, at this time, belo...        sci.space   \n",
      "4   \\nDigi-Key also sells Quad Line Receivers, pa...  sci.electronics   \n",
      "\n",
      "                                          clean_text  \n",
      "0  new kuiper belt object called next one called ...  \n",
      "1  al question certain device al use find number ...  \n",
      "2  quite asking year ago helped ee remote sensing...  \n",
      "3  sky time belong anyone ownership necessary def...  \n",
      "4  also sell quad line receiver part quad line dr...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5e9ae83-cbaa-4d5d-935b-aaefd86a3f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          clean_text            label\n",
      "0  new kuiper belt object called next one called ...        sci.space\n",
      "1  al question certain device al use find number ...  sci.electronics\n",
      "2  quite asking year ago helped ee remote sensing...        sci.space\n",
      "3  sky time belong anyone ownership necessary def...        sci.space\n",
      "4  also sell quad line receiver part quad line dr...  sci.electronics\n"
     ]
    }
   ],
   "source": [
    "clean_data = df[['clean_text', 'label']]\n",
    "print(clean_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6695e4cd-933c-4346-bb75-fbdad38dcbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform train test split\n",
    "X = df['clean_text']\n",
    "y = df['label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ca628ee-ffbe-4909-88a1-0ab450e4532c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1478,) (1478,) (493,) (493,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7ba0a51-ef19-4201-ac9c-0fa52d659217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the data\n",
    "#Bag-of-words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(min_df=10)\n",
    "\n",
    "#Fit and transform the training data\n",
    "X_train_counts = count_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c949a7fb-b4cf-4b3a-883c-e70576fa2ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Transform the test data\n",
    "X_test_counts = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "480ae65b-0582-41da-b7de-7a71d5f5dfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ability  able  absolutely  ac  acceleration  accepted  access  according  \\\n",
      "0        0     0           0   0             0         0       0          0   \n",
      "1        0     0           0   0             0         0       0          0   \n",
      "2        0     0           0   0             0         0       0          0   \n",
      "3        0     0           0   0             0         0       0          0   \n",
      "4        0     0           0   0             0         0       0          0   \n",
      "\n",
      "   account  accuracy  ...  wrong  wrote  yeah  year  yellow  yes  yesterday  \\\n",
      "0        0         0  ...      0      0     0     0       0    0          0   \n",
      "1        0         0  ...      0      0     0     0       0    0          0   \n",
      "2        0         0  ...      0      0     0     0       0    0          0   \n",
      "3        0         0  ...      0      0     0     0       0    0          0   \n",
      "4        0         0  ...      0      0     0     0       0    0          0   \n",
      "\n",
      "   yet  york  zero  \n",
      "0    0     0     0  \n",
      "1    0     0     0  \n",
      "2    0     0     0  \n",
      "3    0     0     0  \n",
      "4    0     0     0  \n",
      "\n",
      "[5 rows x 1716 columns]\n"
     ]
    }
   ],
   "source": [
    "#Display Feature Names\n",
    "\n",
    "counts_df = pd.DataFrame(X_train_counts.toarray(), columns = count_vect.get_feature_names_out())\n",
    "print(counts_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dba321c6-02eb-4bef-a2d3-ccd3c9781880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count_vect_ngram = CountVectorizer(min_df=10, ngram_range = (2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52dd82e6-7c7c-4d69-844f-be083281f598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   al uucp  also available  ames dryden  anonymous ftp  answer question  \\\n",
      "0        0               0            0              0                0   \n",
      "1        0               0            0              0                0   \n",
      "2        0               0            0              0                0   \n",
      "3        0               0            0              0                0   \n",
      "4        0               0            0              0                0   \n",
      "\n",
      "   anybody know  anyone know  appreciated thanks  around office  \\\n",
      "0             0            0                   0              0   \n",
      "1             0            0                   0              0   \n",
      "2             0            0                   0              0   \n",
      "3             0            0                   0              0   \n",
      "4             0            0                   0              0   \n",
      "\n",
      "   available via  ...  would appreciated  would go  would greatly  would help  \\\n",
      "0              0  ...                  0         0              0           0   \n",
      "1              0  ...                  1         0              0           0   \n",
      "2              0  ...                  0         0              0           0   \n",
      "3              0  ...                  0         0              0           0   \n",
      "4              0  ...                  0         0              0           0   \n",
      "\n",
      "   would like  would make  would need  would probably  would think  year ago  \n",
      "0           0           0           0               0            0         0  \n",
      "1           0           0           0               0            0         0  \n",
      "2           0           0           0               0            0         0  \n",
      "3           0           0           0               0            0         0  \n",
      "4           0           0           0               0            0         0  \n",
      "\n",
      "[5 rows x 128 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Fit and transform the training data\n",
    "X_train_counts = count_vect_ngram.fit_transform(X_train)\n",
    "#Transform the test data\n",
    "X_test_counts = count_vect_ngram.transform(X_test)\n",
    "#Display Feature Names\n",
    "\n",
    "counts_ngram_df = pd.DataFrame(X_train_counts.toarray(), columns = count_vect_ngram.get_feature_names_out())\n",
    "print(counts_ngram_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02e18c30-5547-4b27-896c-d189c1f31e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF- IDF \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(max_df = 0.7, min_df = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51d16986-eb2f-49c3-b264-666e0e7c00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train = tfidf_vect.fit_transform(X_train)\n",
    "tfidf_test = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc88914b-4f31-4885-a7a2-e5b51455d01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   able  absolutely   ac  access  according  across  act  action  active  \\\n",
      "0   0.0         0.0  0.0     0.0        0.0     0.0  0.0     0.0     0.0   \n",
      "1   0.0         0.0  0.0     0.0        0.0     0.0  0.0     0.0     0.0   \n",
      "2   0.0         0.0  0.0     0.0        0.0     0.0  0.0     0.0     0.0   \n",
      "3   0.0         0.0  0.0     0.0        0.0     0.0  0.0     0.0     0.0   \n",
      "4   0.0         0.0  0.0     0.0        0.0     0.0  0.0     0.0     0.0   \n",
      "\n",
      "   activity  ...  world  worse     worth     would  write  written  wrong  \\\n",
      "0       0.0  ...    0.0    0.0  0.000000  0.000000    0.0      0.0    0.0   \n",
      "1       0.0  ...    0.0    0.0  0.000000  0.100928    0.0      0.0    0.0   \n",
      "2       0.0  ...    0.0    0.0  0.000000  0.148228    0.0      0.0    0.0   \n",
      "3       0.0  ...    0.0    0.0  0.109279  0.094371    0.0      0.0    0.0   \n",
      "4       0.0  ...    0.0    0.0  0.000000  0.000000    0.0      0.0    0.0   \n",
      "\n",
      "   year  yes  yet  \n",
      "0   0.0  0.0  0.0  \n",
      "1   0.0  0.0  0.0  \n",
      "2   0.0  0.0  0.0  \n",
      "3   0.0  0.0  0.0  \n",
      "4   0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 1152 columns]\n"
     ]
    }
   ],
   "source": [
    "#Display Feature Names \n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_train.toarray(), columns = tfidf_vect.get_feature_names_out())\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba1d1780-ed31-4817-8fac-19467a1c489b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59026369168357"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the classifier and predict the test data\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_counts, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_test_counts )\n",
    "metrics.accuracy_score(y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4cfa775f-d637-4130-8f61-ea33aa59017f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:                  sci.electronics  sci.space\n",
      "sci.electronics               93        156\n",
      "sci.space                     46        198\n"
     ]
    }
   ],
   "source": [
    "labels=['sci.electronics', 'sci.space']\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "#Crate a df from confusion matrix\n",
    "\n",
    "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "\n",
    "#Print the confusion matrix\n",
    "print(\"Confusion Matrix:\", cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed1f809-4828-49dd-b08b-472bbd572ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
